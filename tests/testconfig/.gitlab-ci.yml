#-------------------------------------------------------------------------------
# General rules when to create a pipeline
# N.B. This doesn't say anything about which jobs are run but only determines
# if a pipeline is created.
workflow:
  name: $FOUR_C_PIPELINE_NAME
  rules:
    # for merge requests
  - if: $CI_MERGE_REQUEST_IID
    variables:
      FOUR_C_PIPELINE_NAME: MR pipeline for '$CI_MERGE_REQUEST_SOURCE_BRANCH_NAME'
        # for tags
  - if: $CI_COMMIT_TAG
    variables:
      FOUR_C_PIPELINE_NAME: Tag pipeline for '$CI_COMMIT_TAG'
        # for a merge to the default branch
  - if: $CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
    variables:
      FOUR_C_PIPELINE_NAME: Post-merge pipeline for '$CI_COMMIT_TITLE'
  - if: $CI_PIPELINE_SOURCE == "schedule" && $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
    variables:
      FOUR_C_PIPELINE_NAME: Scheduled pipeline for '$CI_COMMIT_TITLE'
        # if manually triggered with the web interface's "run pipeline"
  - if: $CI_PIPELINE_SOURCE == "web"
    variables:
      FOUR_C_PIPELINE_NAME: Manual pipeline for '$CI_COMMIT_BRANCH'

#-------------------------------------------------------------------------------
# Define global variables for all jobs
variables:
  # Clone repository by default, file have changed attribute.
  GIT_STRATEGY: clone

  GITLAB_PIPELINE_TYPE:
    value: default
    description: Type of pipeline
    options: [default, nightly, coverage, trilinos develop]

  # Variables for docker
  # Whether to build the docker images and push them to the CI registry
  FOUR_C_DOCKER_BUILD_IMAGES: 'False'
  FOUR_C_DOCKER_DEPENDENCIES_HASH: 9a72bec9
  FOUR_C_DOCKER_DEPENDENCIES_IMAGE: 4c-dependencies
  FOUR_C_DOCKER_DEPENDENCIES_IMAGE_WITH_HASH: $FOUR_C_DOCKER_DEPENDENCIES_IMAGE:$FOUR_C_DOCKER_DEPENDENCIES_HASH

  # Variables for Trilinos pipeline
  # The default Trilinos commit ref the Trilinos pipeline is running on
  TRILINOS_PIPELINE_COMMIT_REF: develop
  FOUR_C_DOCKER_DEPENDENCIES_IMAGE_TRILINOS: 4c-dependencies-trilinos:$TRILINOS_PIPELINE_COMMIT_REF

.compute_dependencies_hash: &compute_dependencies_hash
  # compute short hash from contents of folder dependencies and docker (exclude trilinos_config and README.md files)
- COMPUTED_DOCKER_DEPENDENCIES_HASH=`find dependencies docker -not -wholename '*/trilinos_develop/*'
  -not -name 'README.md' -type f -exec sha1sum {} \; | sort | sha1sum | cut -c -8`

before_script:
- git config --global --add safe.directory ${CI_PROJECT_DIR}
- *compute_dependencies_hash
- if [ "${COMPUTED_DOCKER_DEPENDENCIES_HASH}" != "$FOUR_C_DOCKER_DEPENDENCIES_HASH"
  ]; then echo "Docker image version hash does not match the hash of the dependencies.
  You likely need to change the hash to ${COMPUTED_DOCKER_DEPENDENCIES_HASH} and rebuild
  the dependencies." && exit 1; else echo "Running on the correct docker image version
  with hash $FOUR_C_DOCKER_DEPENDENCIES_HASH."; fi


.clean-build-folder-for-nightly-pipelines: &clean-build-folder-for-nightly-pipelines
  # Delete build folder for a clean build for the nightly pipeline
- if [ $GITLAB_PIPELINE_TYPE == "nightly" ]; then echo "Removing build folder"; rm
  -rf ${CI_PROJECT_DIR}/../build/; fi

.build-only: &build-only
- mkdir -p ${CI_PROJECT_DIR}/../build
  # configure
- |
  cd ${CI_PROJECT_DIR}
  ./create-python-venv
  cd ${CI_PROJECT_DIR}/../build
  # We override the launcher arguments here to ensure that ccache is not used during testing
  cmake ${CI_PROJECT_DIR} --fresh --preset=${CMAKE_PRESET} -DCMAKE_C_COMPILER_LAUNCHER="" -DCMAKE_CXX_COMPILER_LAUNCHER=""
- echo Building the following targets ${BUILD_TARGETS}
  # build
- time cmake --build . --target ${BUILD_TARGETS} -- -j `nproc` 2>&1 | tee ${CI_PROJECT_DIR}/build.log

.build-and-test: &build-and-test
- *build-only
  # test
- time ctest -VV -j `nproc` --output-junit ${CI_PROJECT_DIR}/junit_test_summary.xml
  | tee ${CI_PROJECT_DIR}/test.log | grep -e "\*\*\*Failed" -e "\.\.\.   Passed" -e
  " tests passed\,"
- cd ${CI_PROJECT_DIR}

.post_process_log_files: &post_process_log_files
  # sort output of pipeline
- cd ${CI_PROJECT_DIR}/../build
- ${CI_PROJECT_DIR}/utilities/python-venv/bin/python ${CI_PROJECT_DIR}/utilities/sort_pipeline_output.py
  ${CI_PROJECT_DIR}/test.log ${CI_PROJECT_DIR}/test.log
    # Write selection of the output into the terminal and therefore to GITLAB
  # Output 200 lines for any failing test
- grep -B 200 '*Failed\|*Timeout' ${CI_PROJECT_DIR}/test.log || true
- sed -n '/tests passed,/,//p' ${CI_PROJECT_DIR}/test.log
  # Move the log files to the source folder, so it can be found by artifacts.
- cd ${CI_PROJECT_DIR}


# Generic job that builds and tests the project. Derived jobs may configure the details via variables.
.buildtest_base:
  stage: buildtest
  variables:
    # So the old build can be used again.
    GIT_STRATEGY: fetch
    BUILD_TARGETS: full
  script:
  - *clean-build-folder-for-nightly-pipelines
  - *build-and-test
  after_script:
  - *post_process_log_files
  artifacts:
    name: $CI_JOB_NAME-$CI_JOB_ID
    paths:
    - '*.log'
    - junit_test_summary.xml
    reports:
      junit: junit_test_summary.xml
    when: always
    expire_in: 1 day


#-------------------------------------------------------------------------------
# Stages during testing.
#-------------------------------------------------------------------------------

# We use stages just for grouping jobs into different categories. Dependencies are defined using the
# `needs` keyword to reduce the time of a pipeline run, see the Gitlab documentation about Directed
# Acyclic Graph Pipelines (https://docs.gitlab.com/ee/ci/pipelines/pipeline_architectures.html#directed-acyclic-graph-pipelines).
stages:
- build-docker-images
- checkcode
- buildtest
- documentation
- pages

# Check for code style
checkcode:
  stage: checkcode
  image: $CI_REGISTRY_IMAGE/$FOUR_C_DOCKER_DEPENDENCIES_IMAGE_WITH_HASH
  script:
  - echo code check
  - ./create-python-venv
  - bash utilities/code_checks/check_format_and_header
  artifacts:
    name: $CI_JOB_NAME-$CI_JOB_ID
    paths:
    - wrong_*.txt
    when: on_failure
    expire_in: 1 day
  rules:
  - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
  - if: $CI_PIPELINE_SOURCE == "web"
  - if: $GITLAB_PIPELINE_TYPE == "nightly"
  tags:
  - checkcode
  interruptible: true
  needs:
  - job: build_base_dependencies
    optional: true

# Include a pre-configured danger bot job at a fixed version. Verbatim copy of:
# https://gitlab.com/gitlab-org/quality/pipeline-common/-/raw/7.5.1/ci/danger-review.yml
#
# This job supports the following variables:
# - DANGER_GITLAB_API_TOKEN: (Optional) A project access token with `api` scope.
# - GITLAB_DANGERFILES_VERSION: (Optional) Version requirement for `gitlab-dangerfiles`. Latest version if empty.
danger_review:
  image: ruby:3.0
  stage: checkcode
  tags:
  - danger
  interruptible: true
  needs: []
  retry:
    max: 2
    when:
    - unknown_failure
    - api_failure
    - runner_system_failure
    - stuck_or_timeout_failure
  before_script:
  - bundle install --gemfile="./utilities/danger/Gemfile"
  - apt-get update && apt-get install -y python3-venv
    # create  and activate virtual environment
  - python3 -m venv venv
  - . venv/bin/activate
  - pip install wheel
  - pip install -r ${CI_PROJECT_DIR}/utilities/danger/requirements.txt
  script:
  - |
    danger_id=$(echo -n ${DANGER_GITLAB_API_TOKEN} | md5sum | awk '{print $1}' | cut -c5-10);
    BUNDLE_GEMFILE=./utilities/danger/Gemfile bundle exec danger --fail-on-errors=true --verbose --danger_id="${danger_id}" --dangerfile="./utilities/danger/Dangerfile";
  rules:
  - if: $CI_PIPELINE_SOURCE == 'merge_request_event'


# Docker---------------------------------------------------------------------------

codeclimate:
  stage: checkcode
  image: $CI_REGISTRY_IMAGE/$FOUR_C_DOCKER_DEPENDENCIES_IMAGE_WITH_HASH
  script:
  - *clean-build-folder-for-nightly-pipelines
  - mkdir -p ${CI_PROJECT_DIR}/../build && cd ${CI_PROJECT_DIR}/../build
    # configure to get a compilation database: use a unity build for faster parsing of files
  - cmake ${CI_PROJECT_DIR} --fresh -DCMAKE_CXX_COMPILER="clang++" -DFOUR_C_BUILD_READTHEDOCS=OFF
    -DCMAKE_UNITY_BUILD=ON --preset=docker_codeclimate
    # create and activate virtual environment
  - python3 -m venv venv
  - . venv/bin/activate
  - pip install wheel
  - pip install -r ${CI_PROJECT_DIR}/utilities/code_climate/requirements.txt
    # create temporary directory for the yaml files from clang-tidy
  - mkdir clang-tidy-issues
  - ESCAPED_PROJECT_DIR=$(echo "${CI_PROJECT_DIR}" | sed 's/\//\\\//g')
    # run clang tidy and export report
  - run-clang-tidy -j `nproc` -config-file ${CI_PROJECT_DIR}/.clang-tidy -use-color
    -p . -export-fixes ./clang-tidy-issues/ -header-filter "${ESCAPED_PROJECT_DIR}\/.*"
    2>&1 | python ${CI_PROJECT_DIR}/utilities/code_climate/filter_clang_tidy_output.py
    # export clang tidy issues
  - python ${CI_PROJECT_DIR}/utilities/code_climate/export_clang_tidy_issues.py -j
    `nproc` --code-climate-file ${CI_PROJECT_DIR}/codeclimate.json --source-folder
    ${CI_PROJECT_DIR} ./clang-tidy-issues/*.yaml
  - cd ${CI_PROJECT_DIR}
  artifacts:
    paths: [codeclimate.json]
    reports:
      codequality: codeclimate.json
  tags:
  - codeclimate
  needs:
  - job: build_base_dependencies
    optional: true
  interruptible: true
  # Always run this job, no rules needed.


# Full tests with coverage are performed and a coverage report is created
buildtest_coverage:
  stage: buildtest
  image: $CI_REGISTRY_IMAGE/$FOUR_C_DOCKER_DEPENDENCIES_IMAGE_WITH_HASH
  variables:
    BUILD_TARGETS: full
    # enable coverage option of ctest
    CMAKE_PRESET: docker_coverage
  script:
  - *clean-build-folder-for-nightly-pipelines
  - *build-and-test
    # generate the coverage_base.info: the "baseline" coverage data file that contains zero coverage for every instrumented line.
  - lcov --capture --initial --no-external --directory ../build/ --base-directory
    . --output-file coverage_base.info > ${CI_PROJECT_DIR}/coverage_base.log
    # generate the coverage_tests.info based on tests run above
  - lcov --capture --no-external --directory ../build/ --base-directory . --output-file
    coverage_tests.info > ${CI_PROJECT_DIR}/coverage_tests.log
    # combine the baseline coverage with the coverage from the tests
  - lcov --add-tracefile coverage_base.info --add-tracefile coverage_tests.info --output-file
    coverage.info  > ${CI_PROJECT_DIR}/coverage.log
    # remove unwanted files from the coveragre report
  - lcov --remove coverage.info "*/unittests*/*" "*/tests/*" "*/build/*" -o coverage_filtered.info
    > ${CI_PROJECT_DIR}/coverage_filtered.log
    # generate the html version of the coverage report
  - genhtml coverage_filtered.info --legend --demangle-cpp --output-directory coverage_report/
    --title "4C commit $CI_COMMIT_SHORT_SHA" | tee ${CI_PROJECT_DIR}/genhtml_coverage.log
    # add repo link to the commit that the report is based on
  - find coverage_report/ -type f -exec sed -i "s/4C commit $CI_COMMIT_SHORT_SHA/4C
    commit  \<a 
    href=\"https:\/\/gitlab.lrz.de\/baci\/baci\/commit\/$CI_COMMIT_SHA\"\>$CI_COMMIT_SHORT_SHA\<\/a\>/g"
    {} \;
  after_script:
  - *post_process_log_files
  artifacts:
    name: $CI_JOB_NAME-$CI_JOB_ID
    paths:
    - '*.log'
    - coverage_report/
    when: always
    expire_in: 30 days
  rules:
  - if: $GITLAB_PIPELINE_TYPE == "coverage"
  tags:
  - coverage
  needs: []

pages: # job name needs to be pages
  stage: pages
  image: alpine:3.18
  # Download Doxygen and ReadTheDocs from previous documentation stage
  needs:
  - job: build_base_dependencies
    optional: true
  - job: doxygen
    artifacts: true
  - job: readthedocs
  before_script: []
  script:
    # Create python venv for gitlab api and anybadges
  - apk add python3 curl unzip git
  - python3 -m venv venv
  - . venv/bin/activate
  - pip install -r utilities/coverage/requirements.txt
    # Download latest coverage report
  - LATEST_ARTIFACT_URL="`python ./utilities/coverage/get_latest_artifacts_url_by_jobname.py
    $CI_SERVER_URL $ACCESS_TOKEN $CI_PROJECT_ID $CI_DEFAULT_BRANCH buildtest_coverage`"
  - 'curl --location --output coverage_artifacts.zip --header "PRIVATE-TOKEN: $ACCESS_TOKEN"
    "$LATEST_ARTIFACT_URL"'
  - unzip coverage_artifacts.zip -d coverage_artifacts
    # Print measured coverage rate
  - grep "Overall coverage rate:" -A 2 coverage_artifacts/genhtml_coverage.log
    # Create directory that will be published
  - mkdir -p public
    # move created coverage report to public folder (which is the path for GitLab Pages content)
  - mv coverage_artifacts/coverage_report public/coverage_report
    # Move most recent doxygen to public folder
  - mv doxygen public/doxygen
    # Move most recent readthedocs to public folder
  - mv readthedocs public/readthedocs
  - pip install anybadge
    # Create doxygen badge
  - anybadge -l doxygen -v ok --color=green -f public/doxygen.svg
    # Create readthedocs badge
  - anybadge -l readthedocs -v ok --color=green -f public/readthedocs.svg
    # Create website badge
  - anybadge -l website -v ok --color=green -f public/website.svg
  coverage: /lines\.*:\s+\d+\.\d+/
  artifacts:
    # store the public path in artifact
    # this is needed since in a subsequent deploy stage (automatically generated by GitLab)
    # the content of the below artifact is published on GitLab Pages
    paths:
    - public
    expire_in: 1 day
  rules:
  - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
  tags:
  - pages

doxygen:
  stage: documentation
  image: $CI_REGISTRY_IMAGE/$FOUR_C_DOCKER_DEPENDENCIES_IMAGE_WITH_HASH
  script:
  - echo Doxygen
  - *clean-build-folder-for-nightly-pipelines
  - mkdir -p ${CI_PROJECT_DIR}/../build && cd ${CI_PROJECT_DIR}/../build

    # Configure and build (setting FOUR_C_BUILD_READTHEDOCS=OFF, so that we don't need the python virtual environment
  - cmake ${CI_PROJECT_DIR} --fresh --preset=docker -DFOUR_C_BUILD_READTHEDOCS=OFF
  - cmake --build . --target doxygen > ${CI_PROJECT_DIR}/doxygen.log

    # Throw error if there are Latex errors in doxygen comments. Latex errors can be uniquely identified by a line starting with "! ".
  - if grep -qE '^[eE]rror:' ${CI_PROJECT_DIR}/doxygen.log; then echo 'Found at least
    one fatal error during doxygen generation.'; exit 1; else echo 'Doxygen builds
    without fatal errors.'; fi
  - if grep -qE '^! ' ${CI_PROJECT_DIR}/doxygen.log; then echo "Found at least one
    LaTeX error during doxygen build."; exit 1; else echo "Doxygen builds without
    LaTeX errors."; fi
    # Move final doxygen into the base folder for the next stage
  - mv $CI_PROJECT_DIR/../build/doc/doxygen/html $CI_PROJECT_DIR/doxygen
  after_script:
    # Write Selection of the output into the terminal and therefore to GITLAB
  - sed -n '/* Extra verbosity turned on/,/Test project /p' ${CI_PROJECT_DIR}/doxygen.log
    # Write information about possible Latex errors to terminal
  - if grep -qE '^! ' ${CI_PROJECT_DIR}/doxygen.log; then echo "Found at least one
    LaTeX error during doxygen build."; else echo "Doxygen builds without LaTeX errors.";
    fi
  tags:
  - doxygen
  artifacts:
    name: $CI_JOB_NAME-$CI_JOB_ID
    paths:
    - doxygen/
    - doxygen.log
    when: always
    expire_in: 1 day
  interruptible: true
  needs:
  - job: build_base_dependencies
    optional: true
  # Always run this job, no rules needed.

# Build the docker image with all dependencies for 4C
build_base_dependencies:
  stage: build-docker-images
  tags:
  - build-docker-image
  image: docker:20.10.16
  services:
  - docker:20.10.16-dind
  before_script:
  - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
  script:
  - *compute_dependencies_hash
  - echo $CI_REGISTRY_IMAGE
  - echo $CI_REGISTRY
  - echo "Generating 4C dependencies docker image with version-hash $COMPUTED_DOCKER_DEPENDENCIES_HASH"
  - FULL_IMAGE_PATH="$CI_REGISTRY_IMAGE/$FOUR_C_DOCKER_DEPENDENCIES_IMAGE:$COMPUTED_DOCKER_DEPENDENCIES_HASH"
  - docker build --no-cache --tag $FULL_IMAGE_PATH --file docker/Dockerfile .
  - docker push $FULL_IMAGE_PATH
  rules:
  - if: $FOUR_C_DOCKER_BUILD_IMAGES == "True"
    when: manual



buildtest_release:
  extends: .buildtest_base
  image: $CI_REGISTRY_IMAGE/$FOUR_C_DOCKER_DEPENDENCIES_IMAGE_WITH_HASH
  variables:
    CMAKE_PRESET: docker
  tags:
  - buildtest
  needs:
  - job: build_base_dependencies
    optional: true
  rules:
  - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
  - if: $CI_PIPELINE_SOURCE == "web"
  - if: $GITLAB_PIPELINE_TYPE == "nightly"
  interruptible: true

buildtest_release_asan:
  extends: .buildtest_base
  image: $CI_REGISTRY_IMAGE/$FOUR_C_DOCKER_DEPENDENCIES_IMAGE_WITH_HASH
  variables:
    CMAKE_PRESET: docker_asan
  after_script:
  - *post_process_log_files
  - ${CI_PROJECT_DIR}/utilities/grep_asan_failures.sh ${CI_PROJECT_DIR}/test.log >
    ${CI_PROJECT_DIR}/asan_summary.log
  tags:
  - buildtest
  needs:
  - job: build_base_dependencies
    optional: true
  rules:
  - if: $GITLAB_PIPELINE_TYPE == "nightly"
  interruptible: true


buildtest_release_with_assertions:
  extends: .buildtest_base
  image: $CI_REGISTRY_IMAGE/$FOUR_C_DOCKER_DEPENDENCIES_IMAGE_WITH_HASH
  variables:
    CMAKE_PRESET: docker_release_assertions
  tags:
  - buildtest
  needs:
  - job: build_base_dependencies
    optional: true
  rules:
  - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
  - if: $CI_PIPELINE_SOURCE == "web"
  - if: $GITLAB_PIPELINE_TYPE == "nightly"
  interruptible: true

build_verify_headers:
  image: $CI_REGISTRY_IMAGE/$FOUR_C_DOCKER_DEPENDENCIES_IMAGE_WITH_HASH
  variables:
    NUMBER_OF_BUILD_THREADS: '10'
  tags:
  - verify-headers
  needs:
  - job: build_base_dependencies
    optional: true
  stage: checkcode
  script:
  - *clean-build-folder-for-nightly-pipelines
  - mkdir -p ${CI_PROJECT_DIR}/../build
    # configure
  - |
    cd ${CI_PROJECT_DIR}
    ./create-python-venv
    cd ${CI_PROJECT_DIR}/../build
    # Configure verification of header sets.
    cmake ${CI_PROJECT_DIR} --fresh --preset=docker -DCMAKE_VERIFY_INTERFACE_HEADER_SETS="ON"
    # verify headers
  - time cmake --build . --target all_verify_interface_header_sets -- -j `nproc` 2>&1
    | tee ${CI_PROJECT_DIR}/verify_headers.log
  artifacts:
    name: $CI_JOB_NAME-$CI_JOB_ID
    paths:
    - '*.log'
    when: on_failure
    expire_in: 1 day
  rules:
  - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
  - if: $CI_PIPELINE_SOURCE == "web"
  - if: $GITLAB_PIPELINE_TYPE == "nightly"
  interruptible: true

readthedocs:
  stage: documentation
  image: $CI_REGISTRY_IMAGE/$FOUR_C_DOCKER_DEPENDENCIES_IMAGE_WITH_HASH
  script:
  - echo Create ReadTheDocs documentation
    # loading python virtual environment
  - cd ${CI_PROJECT_DIR}
  - ./create-python-venv
  - *clean-build-folder-for-nightly-pipelines
  - mkdir -p ${CI_PROJECT_DIR}/../build && cd ${CI_PROJECT_DIR}/../build

    # Configure and build
  - cmake ${CI_PROJECT_DIR} --fresh --preset=docker
  - cmake --build . --target readthedocs | tee ${CI_PROJECT_DIR}/readthedocs.log

    # Throw error if there are errors (or Warnings treated as errors) in readthedocs comments.
  - if grep -qE 'ERROR|treated as error:' ${CI_PROJECT_DIR}/readthedocs.log; then
    echo 'Found at least one fatal error during readthedocs generation.'; exit 1;
    else echo 'ReadTheDocs documentation builds without fatal errors.'; fi
    # Move final readthedocs into the base folder for the next stage
  - mv $CI_PROJECT_DIR/../build/doc/readthedocs/html $CI_PROJECT_DIR/readthedocs
  after_script:
    # Write Selection of the output into the terminal and therefore to GITLAB
  - sed -n '/* Extra verbosity turned on/,/Test project /p' ${CI_PROJECT_DIR}/readthedocs.log
  tags:
  - readthedocs
  artifacts:
    name: $CI_JOB_NAME-$CI_JOB_ID
    paths:
    - readthedocs/
    - readthedocs.log
    when: always
    expire_in: 1 day
  interruptible: true
  needs:
  - job: build_base_dependencies
    optional: true
  # Always run this job, no rules needed.

# The Trilinos pipeline with a specific commit ref runs via a schedule or when manually triggered
build_base_dependencies_trilinos_develop:
  stage: build-docker-images
  tags:
  - build-docker-image
  image: docker:20.10.16
  services:
  - docker:20.10.16-dind
  before_script:
  - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
  script:
  - echo "Generating 4C dependencies docker image with Trilinos version $TRILINOS_PIPELINE_COMMIT_REF"
  - FULL_IMAGE_PATH="$CI_REGISTRY_IMAGE/$FOUR_C_DOCKER_DEPENDENCIES_IMAGE_TRILINOS"
  - docker build --no-cache --build-arg NPROCS=`nproc` --build-arg TRILINOS_VERSION="$TRILINOS_PIPELINE_COMMIT_REF"
    --tag $FULL_IMAGE_PATH --file docker/trilinos_develop/Dockerfile .
  - docker push $FULL_IMAGE_PATH
  rules:
  - if: $GITLAB_PIPELINE_TYPE == "trilinos develop"
  interruptible: true

buildtest_release_trilinos_develop:
  extends: .buildtest_base
  image: $CI_REGISTRY_IMAGE/$FOUR_C_DOCKER_DEPENDENCIES_IMAGE_TRILINOS
  variables:
    CMAKE_PRESET: docker
  tags:
  - buildtest
  needs:
  - job: build_base_dependencies_trilinos_develop
    optional: true
  rules:
  - if: $GITLAB_PIPELINE_TYPE == "trilinos develop"
  interruptible: true

buildtest_release_with_assertions_trilinos_develop:
  extends: .buildtest_base
  image: $CI_REGISTRY_IMAGE/$FOUR_C_DOCKER_DEPENDENCIES_IMAGE_TRILINOS
  variables:
    CMAKE_PRESET: docker_release_assertions
  tags:
  - buildtest
  needs:
  - job: build_base_dependencies_trilinos_develop
    optional: true
  rules:
  - if: $GITLAB_PIPELINE_TYPE == "trilinos develop"
  interruptible: true
